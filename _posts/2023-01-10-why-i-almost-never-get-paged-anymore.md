## Why I Almost Never Get Paged Anymore

### Gone are the hard drives
The other night I awoke to the barbershop quartet piece “Gone are the hard drives of all those golden days” (I love PagerDuty’s custom alarm tones). It was 3:00 am and I had been paged by an automated alarm. I squinted at the blinding light of my phone and crawled out of bed to retrieve my work laptop before burying myself in the covers once again, but with my head propped up with another pillow. I followed the links to the grafana dashboard, logs, and followed the prescribed steps in the runbook to investigate and triage the problem. 

While awaiting an approval on a PR that might fix the problem I noticed something peculiar–the service that was experiencing 5xxs was only experiencing them for one of the four replicas. This led me to believe the problem was that this particular pod had gotten into a bad state, and that the service itself besides that was okay.

I restarted the pod, then watched as the rate of 5xxs dropped from 8% to 0%. At this point the incident was resolved. I wrote a few things in Slack for us to review on Monday and updated the runbook in case we run into the same incident again. Now I could finally close my laptop and fall asleep.

### How to reduce incidents
When I awoke I spoke with my wife about the incident and it occurred to me that I rarely get paged anymore, and I very rarely get paged at night. The last time I think I had been paged in the middle of the night must have been a year before, and I only get paged once every few months besides that and it’s usually during the work day when someone deploys a change.

It is such a relief to rarely be paged! As I thought about this I started to contemplate on what decisions we had made in the Drift AI Lab to reduce the number of times we get paged. What did we do that made our services that much more robust? I attribute our reduction in incidents to a few things:
- Adding logging, testing, metrics, alarming to services
- Feedback from experienced engineers
- Simpler services

I will cover the first one in this article.

### Adding logging, testing, metrics, alarming to all our services
This was a major shift for me. We always had tools for all these things at Drift, but as a newer NLP Engineer I had zero experience with automated testing. When I first started working at Drift almost all our work in the AI lab was done in a single large repo that had grown more complex with each year. There were massive libraries that interacted with each other and you had to know how everything fit together in order to work on any part of it. Back then the general development process was to build a dialogue agent locally and run it in a dev environment locally. After making a change I’d build an agent using the data from one or two of the clients and then start a conversation with the agent using roughly the same set of inputs I would always use. If my tests failed I’d set some breakpoints, investigate, rebuild, and retest until I was satisfied my changes probably wouldn’t cause any problems. Besides untangling some of the spaghetti that began to grow, what we desperately needed was testing.

My boss was the first to take action on this. The first iteration of testing began as a behavioral end-to-end test where each client’s dialogue agent would undergo a series of conversations in a test environment, and any errors were logged in the output of the test. This was a great first step because it was a simple way to automate what we were all doing anyways with running tests manually after each change. These automated tests were superior in the way that they would test more things than engineers would test, test edge cases, and test consistently. They were also much faster. Upon the test’s release we immediately found there were many client’s dialogue agents that were failing already! So the first action was to get all agents passing tests. It took a few weeks of bug squashing and finally all agents were passing the end-to-end behavioral tests!

This was a massive step forward, but we still needed more. The next step was unit tests, and we added these as we started to create new services to replace large libraries in the old repo. At this point in my career I had over three years of experience and was ashamed to say I didn’t have any experience in testing beyond being an observer and user of the behavioral tests that were just created. So I started small. I looked at what tools other engineers were using in Python and what their tests looked like. I think my first tests I created by copying the tests package from another team’s repo and retrofitting the tests to match the code in my repo. Writing tests was ugly and awkward at first, and it hardly made sense to me, but over time my confidence waxed as I googled how to test and write tests.

As time went on I had more opportunities to create new services myself and to contemplate what decisions I could make at the beginning to make our services more robust. I started by saying to myself that from now on, every service I create will have testing–it is a must. During this time I started to think about what else could help prevent getting paged late at night or having to get pulled away from my current work to put out a fire and concluded that logging, metrics, and alarms were the other missing pieces that I needed to include.

Over the following months and years I would create a new service or inherit one from another team or engineer and see how I could add tests, metrics, alarms, and logging.

### False start
As time went on, we had engineers leave and new engineers join and soon the size of our AI lab doubled in size. I started to become an expert on testing relative to the other members of the lab so I decided now was the time to start spreading my knowledge and get others to adopt these best practices.

I asked the head of our AI lab for some time during our next AI lab meeting to discuss best practices for testing, answer any questions, and show other engineers examples they could use when creating tests for themselves. This seems to have been okay. There were a good amount of questions and engineers seemed interested, but as the weeks and months passed it was clear that adoption was slower than ideal.
At this point I realized that we'd need to do something more to increase the adoption of testing, alarming, metrics, and logging among the AI Lab, but it took me a while to find out what that might be.

[comment]: <> (that the best thing I could do to encourage the adoption of testing was to remove as much friction as possible, and the way I would do this is through templating.)

### Templating to the rescue
We have a templating repo used for creating new services at Drift that uses Yeoman, a powerful yet simple tool for generating code from templates. Templating is superior to boilerplate because it minimizes the chance for errors in adapting the boilerplate code to your use case and speeds up the time to get your code created in general. If you find yourself using example repositories or modules as boilerplate for new code by copying that code over and doing a bunch of find/replace actions this is a telltale sign that you could benefit from templating instead. I found we had a template for Python FastAPI services that had been created a year before and recently refactored by another engineer, but had apparently rarely been used. How had I not heard of this before?

The template created a FastAPI service along with the terraform code to create an RDS instance and the circleCI code to build and deploy it. The template was great and already saved me at least an hour of time compared to creating my repo from boilerplate, but there were some things missing that I wanted to add: testing, development and testing environments with a local database instance, metrics, and logging. I added all of these to my new repo and then to the template, and when I was done it was a thing of beauty–have you ever had one of those moments where you really feel like you’ve done something you are proud of and are excited to share? When I was done I tested out the new template by creating a new service using the new template called “test-service” and saw that we now had a template that enabled you to create a new service in under five minutes that has: unit testing and integration testing (integration testing is done using a containerized MySQL and FastAPI stack using Test Containers, but that deserves its own story), logging, metrics, terraform, and a circleCI template that now ran the same unit and integration tests as part of its build–no more excuses due to failing to forget to run tests since they would be run for you.
The next time I needed to create a new service I used the template and was able to get a simple FastAPI service up and running in Kubernetes in minutes. The project was a success!

### Second time's the charm
A few weeks later I had the chance to speak during AI lab meeting again and gave a demo of the templating tool and just how easy it was to create a service with testing, logging, metrics, and all the infrastructure code handled automatically. This time there was much more interest from my fellow NLP and ML engineers. This was something that would save them time from the very beginning and something that felt like more of a tool than an extra burden or thing they felt they needed to do before getting to the rest of their work. Not everyone values the testing, logging, and monitoring to the same degree, but I had found a way to make adopting the use of these tools easier than not doing so. Engineers could create a new repo from a template and have testing from right out of the gate, or they could go the harder route and create a repo from scratch or copy code from some existing repo, but those options would take more work than simply using the templating tool.

I can't be certain which efforts contributed the most or least to our reduction in incidents over the past few years, but as for me I believe that automated testing, logging, metrics, and alarming has had a large impact on it.

It has been a long journey from being a green NLP engineer with embarrassingly little knowledge and experience with testing to being one of its strongest advocates in the AI lab. I, for one am so happy for every night I get to sleep all the way through and not be paged. There’s a lot of arguments for testing and making your services more robust in general, but this is one I don’t hear as often as I think I should: Do it for yourselves, engineers. Make your systems so robust that you don’t worry about refactoring. Add tests and make those tests run as part of an automated CI/CD build process so you know builds that fail tests won’t be deployed. Add tests, metrics, and alarms when you fix bugs so that you will find those bugs before any customer does, before any alarm goes off, and ideally before you even deploy the code.

